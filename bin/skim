#!/usr/bin/env python
# encoding=utf8

from collections import OrderedDict
import sys, json, glob, re, time, os, time, csv

# File index for progression reporting.
fileIndex = 0

# Color and formatting
def bold (string):
    return "\033[1;89m%s\033[0m " % string

def italic (string):
    return "\033[2;89m%s\033[0m " % string

def green (colored):
    return "\033[92m%s\033[0m " % colored

def red (colored):
    return "\033[91m%s\033[0m " % colored

def gray (colored):
    return "\033[2;88m%s\033[0m " % colored

def yellow (colored):
    return "\033[93m%s\033[0m " % colored

def blue (colored):
    return "\033[94m%s\033[0m " % colored

def magenta (colored):
    return "\033[95m%s\033[0m " % colored

def teal (colored):
    return "\033[96m%s\033[0m " % colored

def divider():
    return " " + gray(("-" * (int(os.popen("tput cols").read()) - 2)))

# Check to see if a flag is set
def isset (arg):
    return arg in sys.argv

# Get a flags value
def get (arg):
    if isset(arg):
        if arg in sys.argv and len(sys.argv) > (sys.argv.index(arg) + 1):
            return sys.argv[sys.argv.index(arg) + 1]

# Creates a visual progression, used within the for-loop
pwidth = int(os.popen("tput cols").read()) - 14
picon = "#"

def progress():
    # Increment the boundless index
    global fileIndex
    fileIndex += 1

    # The value of how many progress signs (#) can 
    # be shown out of the max length (bar_len)
    filled_len = int(round(pwidth * fileIndex / float(totalLines)))
    
    # Rounds the division between the boundless 
    # index and the total lines being scanned
    percents = round(100.0 * fileIndex / float(totalLines), 1)
    
    # Presentation of the data above as a progress bar.
    bar = picon * filled_len + '-' * (pwidth - filled_len)
    sys.stdout.write('\r' + green('→') + '[%s] %s%s ' % (bar, percents, '%'))
    sys.stdout.flush()

# Display a help menu
if isset("-h") or isset("--help"):
    print green("→ [--file, -f]") + " Specify a file or glob of files"
    sys.exit(0)

# Load the configuration file.
data = json.load(open('config.json'), object_pairs_hook=OrderedDict)

# Collection of limitation and exclusion rules
# as well as template columns to report on and
# the collection that stores all results
limitations = {}
exclusions = {}
template = {}
collection = []

# Build limitation rules
for i in data["limitations"]:
    if data["limitations"][i]:
        limitations[data["limitations"].keys().index(i)] = data["limitations"][i]
        pass

# Build exclusion rules
for i in data["exclusions"]:
    if data["exclusions"][i]:
        exclusions[data["exclusions"].keys().index(i)] = data["exclusions"][i]
        pass

# Build the template
for i in data["template"]:
    template[data["template"].keys().index(i)] = data["template"][i]
    pass

# Collect only the indexes of columns to report on.
templateKeys = [i for i, x in enumerate(template.values()) if x]
templateNames = []
for x in templateKeys:
    templateNames.append(data["template"].keys()[x])

# If no template columns
# have been set, abort
# the process.
if not templateKeys:
    print red("→ No template columns have been set")
    sys.exit(0)

# If no limitation are 
# discovered, print out message 
# and exit process.
if not limitations and not exclusions:
    print red("→ No limitations or exclusions specified.")
    sys.exit(0)

# Log file glob
if get("--file") or get("-f") or get("--files"):
    logFiles = glob.glob(get("--file") or get("-f") or get("--files"))
else:
    logFiles = glob.glob('logs/*.log')

# If no log files are found from 
# the glob, print out message 
# and exit process.
if not logFiles:
    print red("→ No log files found.")
    sys.exit(0)

# The variable holding the count of total lines
# doesn't account (pun) for commented lines.
totalLines = 0
for logFile in logFiles:
    totalLines += int(re.sub(r'  +| .*', '', os.popen("wc -l " + logFile).read())) + 1

# Report the number of log files 
# and the total lines counted
print green("→ Total log files:")  + str(len(logFiles))
print green("→ Total lines:")  + "{:,}".format(totalLines)

sys.stdout.write('\r' + green('→ Working:'))
sys.stdout.flush()

# Start the clock
start = time.time()

# Iteration over logfiles
for logFile in logFiles:
    with open(logFile) as f:
        for line in f:
            # Skip lines that are there for column declaration
            if re.compile("^#").match(line):
                continue

            # Split the data with a space delimiter
            # Unlike a CSV, the data is spaced out
            data = line.split(' ')

            # Skip lines that are null, or just a null line.
            # Also skip requests to "/".
            # if re.compile("^-$").match(line[6]) or data[6] == "/":
            if re.compile("^-$").match(line[6]):
                continue

            # If the value is within the same index item of the line data
            # negate the loop and continue the parent loop.
            breakout = False
            for i, d in enumerate(exclusions):
                if breakout is True:
                    break

                for v in exclusions[d]:
                    # If the value is within the same index item of the line data
                    # or matches the current expression, breakout from 
                    # this line to continue on to the next set.
                    if re.match("^\/.*?\/$", v):
                        if re.search( re.sub('^\/|\/$', '', v), data[d].decode("ascii", "ignore") ):
                            breakout = True
                            break
                    elif v in data[d].decode("ascii", "ignore"):
                        breakout = True
                        break

            if breakout is True:
                continue

            truths = 0
            for i, d in enumerate(limitations):
                for v in limitations[d]:
                    # If the value is within the same index item of the line data
                    # iterate the truth count and break the current for-loop.
                    # If the string looks like an expression, run it as such.
                    if re.match("^\/.*?\/$", v):
                        if re.search( re.sub('^\/|\/$', '', v), data[d].decode("ascii", "ignore") ):
                            truths += 1
                            break
                    elif v in data[d].decode("ascii", "ignore"):
                        truths += 1
                        break

            # Ensure the number of known truths is equivalent
            # to the number of limitations keys.
            if truths == len(limitations):
                collection.append(data)

# Report that the heavy part is now finished.
sys.stdout.write('\r' + green('→ Working:') + 'done')
sys.stdout.flush()

# End the clock and report the time 
# exhausted during this process.
end = time.time()
print "\n" + divider()

resultCount = "{:,}".format(len(collection))

if resultCount == "0" or resultCount == 0:
    print red("→ No matches were found.")
    print red("→ No CSV file will be saved.")
else:
    # Report the number of matches found
    print green("→ Matches found:") + resultCount

    # Save contents out to CSV report file.
    filename = str(time.strftime("reports/%m-%d-%Y_%H:%M:%S.csv"))

    # Set CSV writer
    w = csv.writer(open(filename, "w"))

    # Write the headers being used for reporting
    w.writerow(["occurrences"] + templateNames)

    # For each row, split by space and only collect on fields 
    # that were specified in the template config settings.
    rows = []
    for x in collection:
        row = []
        for key, val in enumerate(x):
            if key in templateKeys:
                row.append(val)
        rows.append(row)

    # Collection of occurances of every reported row
    # in the rows list above
    occurrences = []

    # Collection of items also used in reference 
    # for excluding duplicate results.
    lines = []

    # Collect duplicate count in the same order as
    # lines are singled out from their duplicates.
    for x in rows:
        if not x in lines:
            lines.append(x)
            occurrences.append(rows.count(x))

    # Merge occurance values into the first index of 
    # each item accoringly. A weird merge basically.
    for index, value in enumerate(lines):
        lines[index] = [occurrences[index]] + value

    # Sort the lines by their initial value, the 
    # occurence count. Then write the lines 
    # to the CSV report file.
    for x in sorted(lines, reverse=True):
        w.writerow(x)

    # Report the number of duplicates removed
    percentage = "{0:.0f}%".format(float(len(collection) - len(lines)) / len(collection) * 100.0)
    overall = "{:,}".format(len(collection) - len(lines))
    print green("→ Duplicates removed:") + overall + " - " + percentage + " reduction"

    # Report the number of lines being reported
    print green("→ Total lines written:") + "{:,}".format(len(lines))

    # Notify the user of save completion
    print green("→ Saving results to:") + filename

print divider()
print green("→ Completed in:") + str(round(float(end - start), 2)) + ' seconds'