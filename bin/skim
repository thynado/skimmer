#!/usr/bin/env python
# encoding=utf8

import multiprocessing as mp
from collections import OrderedDict
import string, sys, json, glob, re, time, os, time, csv, multiprocessing

# Color and formatting
def bold (string):
    return "\033[1;89m%s\033[0m " % string

def italic (string):
    return "\033[2;89m%s\033[0m " % string

def green (colored):
    return "\033[92m%s\033[0m " % colored

def red (colored):
    return "\033[91m%s\033[0m " % colored

def gray (colored):
    return "\033[2;88m%s\033[0m " % colored

def yellow (colored):
    return "\033[93m%s\033[0m " % colored

def blue (colored):
    return "\033[94m%s\033[0m " % colored

def magenta (colored):
    return "\033[95m%s\033[0m " % colored

def teal (colored):
    return "\033[96m%s\033[0m " % colored

# Check to see if a flag is set
def isset (arg):
    return arg in sys.argv

# Get a flags value
def get (arg):
    if isset(arg):
        if arg in sys.argv and len(sys.argv) > (sys.argv.index(arg) + 1):
            return sys.argv[sys.argv.index(arg) + 1]

# Display a help menu
if isset("-h") or isset("--help"):
    print green("→ --file, -f") + " Specify a file or glob of files"
    print green("→ --help, -h") + " This help menu"
    sys.exit(0)

# Load the configuration file.
data = json.load(open('config.json'), object_pairs_hook=OrderedDict)

# Collection of limitation and exclusion rules
# as well as template columns to report on and
# the collection that stores all results
limitations = {}
exclusions = {}
template = {}
collection = []

# Build limitation rules
for i in data["limitations"]:
    if data["limitations"][i]:
        limitations[data["limitations"].keys().index(i)] = data["limitations"][i]
        pass

# Build exclusion rules
for i in data["exclusions"]:
    if data["exclusions"][i]:
        exclusions[data["exclusions"].keys().index(i)] = data["exclusions"][i]
        pass

# Build the template
for i in data["template"]:
    template[data["template"].keys().index(i)] = data["template"][i]
    pass

# Collect only the indexes of columns to report on.
templateKeys = [i for i, x in enumerate(template.values()) if x]
templateNames = []
for x in templateKeys:
    templateNames.append(data["template"].keys()[x])

# If no template columns
# have been set, abort
# the process.
if not templateKeys:
    print red("→ No template columns have been set")
    sys.exit(0)

# If no limitation are 
# discovered, print out message 
# and exit process.
if not limitations and not exclusions:
    print red("→ No limitations or exclusions specified.")
    sys.exit(0)

# Log file glob
if get("--file") or get("-f") or get("--files"):
    logFiles = glob.glob(get("--file") or get("-f") or get("--files"))
else:
    logFiles = glob.glob('logs/*.log')

logFiles.sort()

# If no log files are found from 
# the glob, print out message 
# and exit process.
if not logFiles:
    print red("→ No log files found.")
    sys.exit(0)

if len(logFiles) > 5:
    try:
        limit = raw_input(green("→ How many log files do you want used") + gray("(" + str(len(logFiles)) + " found)"))
        if limit:
            logFiles = logFiles[:int(limit)]
    except ValueError:
        print(red("→ That's not a number. Please try again."))
        sys.exit(0)

# The variable holding the count of total lines
# doesn't account (pun) for commented lines.
totalLines = 0
for logFile in logFiles:
    totalLines += int(re.sub(r'  +| .*', '', os.popen("wc -l " + logFile).read())) + 1

# Report the number of log files 
# and the total lines counted
print green("→ Reading") + "{:,}".format(totalLines) + green(" lines across") + str(len(logFiles)) + green(" log files")

# Count available cores, then subtract one 
# out of the kindness of our hearts.
cores = int(os.popen("sysctl -n hw.ncpu").read()) - 1
print green("→ Total processes created:")  + str(cores)

sys.stdout.write('\r' + green('→ Working:'))
sys.stdout.flush()

# Start the clock
start = time.time()

# Once all threads are completed run the following to save to a report file
def report (collection):
    global start, templateNames

    # Report that the heavy part is now finished.
    sys.stdout.write('\r' + green('→ Working:') + 'done')
    sys.stdout.flush()

    # End the clock and report the time 
    # exhausted during this process.
    print "\n" + green("→ Completed in:") + str(round(float(time.time() - start), 2)) + ' seconds'

    resultCount = "{:,}".format(len(collection))

    if resultCount == "0" or resultCount == 0:
        print red("→ No matches were found.")
        print red("→ No CSV file will be saved.")
    else:
        # Report the number of matches found
        print green("→ Matches found:") + resultCount

        # Notify of the next heavy lifting piece.
        sys.stdout.write('\r' + green('→ Removing duplicates:'))
        sys.stdout.flush()

        # Save contents out to CSV report file.
        filename = str(time.strftime("reports/%m-%d-%Y_%H:%M:%S.csv"))

        # Set CSV writer
        file = open(filename, "w")
        writer = csv.writer(file)

        # # For each row, split by space and only collect on fields 
        # # that were specified in the template config settings.
        for index, item in enumerate(collection):
            updated = []
            
            for key, value in enumerate(item):
                if key in templateKeys:
                    updated.append(value)
            
            collection[index] = updated

        try:
            # Save the collection to the report file
            for x in collection:
                writer.writerow(x)
        finally:
            file.close()
            del writer

            # Write the headers being used for reporting
            header = ",".join(["occurrences"] + templateNames)
            os.popen("LC_ALL=C sort " + filename + " | LC_CTYPE=C uniq -c | sed 's/^ *\\([0-9][0-9]*\) /\\1,/' | LC_ALL=C sort -nrk 1,1 -o " + filename)
            os.popen("echo '" + header + "' | cat - " + filename + " > temp && mv temp " + filename)

            # Tell the user that all is done.
            sys.stdout.write('\r' + green('→ Removing duplicates:') + 'done')
            sys.stdout.flush()

            originalCount = len(collection)
            writtenCount = int(re.sub(r'  +| .*', '', os.popen("wc -l " + filename).read())) - 1

            # Report the number of duplicates removed
            percentage = "{0:.02f}%".format(float(originalCount - writtenCount) / originalCount * 100.0)
            overall = "{:,}".format(originalCount - writtenCount)
            print green("\n→ Removed") + overall + green(" lines -") + percentage + green(" reduction in size.")

            # Report the number of lines being reported
            print green("→ Total lines written:") + "{:,}".format(writtenCount)

            # Notify the user of save completion
            print green("→ Saving results to:") + filename

# Collect file data
def collect (file):
    response = []
    with open(file) as f:
        for line in f:
            # Skip lines that are there for column declaration
            if re.compile("^#").match(line):
                continue

            # Split the data with a space delimiter
            # Unlike a CSV, the data is spaced out
            data = line.split(' ')

            # Skip lines that are null, or just a null line.
            # Also skip requests to "/".
            # if re.compile("^-$").match(line[6]) or data[6] == "/":
            if re.compile("^-$").match(line[6]):
                continue

            # If the value is within the same index item of the line data
            # negate the loop and continue the parent loop.
            breakout = False
            for i, d in enumerate(exclusions):
                if breakout is True:
                    break

                for v in exclusions[d]:
                    # If the value is within the same index item of the line data
                    # or matches the current expression, breakout from 
                    # this line to continue on to the next set.
                    if re.match("^\/.*?\/$", v):
                        if re.search( re.sub('^\/|\/$', '', v), data[d].decode("ascii", "ignore") ):
                            breakout = True
                            break
                    elif v in data[d].decode("ascii", "ignore"):
                        breakout = True
                        break

            if breakout is True:
                continue

            truths = 0
            for i, d in enumerate(limitations):
                for v in limitations[d]:
                    # If the value is within the same index item of the line data
                    # iterate the truth count and break the current for-loop.
                    # If the string looks like an expression, run it as such.
                    if re.match("^\/.*?\/$", v):
                        if re.search( re.sub('^\/|\/$', '', v), data[d].decode("ascii", "ignore") ):
                            truths += 1
                            break
                    elif v in data[d].decode("ascii", "ignore"):
                        truths += 1
                        break

            # Ensure the number of known truths is equivalent
            # to the number of limitations keys.
            if truths == len(limitations):
                response.append(data)

    return response

# Start pool and create X processes.
pool = multiprocessing.Pool(processes=cores)

# Collect the results after all have completed.
results = [pool.apply_async(collect, args=(x,)) for x in logFiles]

# Merge results into a single list/collection
for sublist in [p.get() for p in results]:
    for item in sublist:
        collection.append(item)

# Report on the collection
report(collection)